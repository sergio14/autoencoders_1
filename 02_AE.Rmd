
---
title: "Features_Exploring"
output:
  html_document: default
  html_notebook:
    code_folding: hide
    fig_height: 6
    fig_width: 10
---







```{r  message=FALSE, warning=FALSE, setup}
# List of packages for session
.packages = c("autoencoder", "SAENET","ripa", "dplyr", "jpeg")

# Install CRAN packages (if not already installed)
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])

# Load packages into session 
lapply(.packages, require, character.only=TRUE)

```

### Seteo del alineamiento a ser analizado
```{r warning= FALSE, message=FALSE }
#Archivos a utilizar y directorio
input="/home/ubuntu/DATA/02_muestras_testeo/"
file="muestra.fasta"

system(paste0("mkdir ",input,"outputs"))
output=paste0(input,"outputs/") 
#lugar=setwd(getwd())

```



```{r warning= FALSE, message=FALSE }
## Matriz de Comparaci√≥n
####################################################################################################################
### 1 Simple Autoencoder
####################################################################################################################
compM<-readRDS(file=paste0(output,"CompMatrixM3.rds"))
varID<-readRDS(paste0(output,"varID.rds"))

#transpose
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
data1<-apply(compM,2,range01)


#Run atuoencoder
set.seed(2016)
n=nrow(data)
train<-sample(1:n,10,FALSE)
fit<-autoencode(X.train=data1[,train],
X.test=NULL,
nl=3,
N.hidden=5,
unit.type="logistic",
lambda=1e-5,
beta=1e-5,
rho=0.07,
epsilon=0.1,
max.iterations=100,
optim.method=c("BFGS"),
rel.tol=0.01,
rescale.flag=TRUE,
rescaling.offset=0.001)

## Error
fit$mean.error.training.set


```






```{r warning= FALSE, message=FALSE }
 attributes ( fit )
fit$mean.error.training.set

#low dimension data
features<-predict(fit,X.input=data1[,
train],hidden.output=TRUE)
features$X.output

#reconstruction
pred<-predict(fit,X.input=data1[,train
],hidden.output=FALSE)
hidden <-pred$X.output



library(plotly)

pc<-as.data.frame(hidden[,1:3])%>%
                      dplyr::rename(Dim1=V1,
                             Dim2=V2,
                             Dim3=V3)

p <- plot_ly(pc, x = ~Dim1, y = ~Dim2, z = ~Dim3, color = ~as.factor(varID$V2[2:length(varID$V2)]),text = varID$V2[2:length(varID$V2)]) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'Dim1'),
                     yaxis = list(title = 'Dim2'),
                     zaxis = list(title = 'Dim3')))

p


```








####  STACKED AUTOENCODERS
```{r warning= FALSE, message=FALSE }
####################################################################################################################
### 1 Simple Autoencoder
####################################################################################################################
compM<-readRDS(file=paste0(output,"CompMatrixM3.rds"))
varID<-readRDS(paste0(output,"varID.rds"))

#transpose
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
data<-apply(compM,2,range01)



#sample of 10 cases
set.seed (2016)
n = nrow(data)
train <- sample(1:n , 100 , FALSE )

```






```{r warning= FALSE, message=FALSE }
library(SAENET)

fit<-SAENET.train(X.train=data[train,],
n.nodes=c(5,4,2),  #3 hidden layers with 5, 4 and 2 nodes
unit.type="logistic",
lambda=1e-5,
beta=1e-5,
rho=0.07,
epsilon=0.1,
max.iterations=100,
optim.method=c("BFGS"),
rel.tol=0.01,
rescale.flag=TRUE,
rescaling.offset=0.001)


```



```{r warning= FALSE, message=FALSE }
#output from last layer
plot(fit[[3]]$X.output[,1],fit[[3]]$X.output[,1])
```





### SDA stacking Denoising Autoencoders

```{r warning= FALSE, message=FALSE }
require(RcppDL)
require("ltm")
data(Mobility)
data<-Mobility


set.seed(17)
n=nrow(data)
sample<-sample(1:n,1000,FALSE)
data<-as.matrix(Mobility[sample,])
n=nrow(data)
train<-sample(1:n,800,FALSE)

x_train<-matrix(as.numeric(unlist(data[train,])),nrow=nrow(data[train,]))
x_test<-matrix(as.numeric(unlist(data[-train,])),nrow=nrow(data[-train,]))

x_train<-x_train[,-3]
x_test<-x_test[,-3] 

```


```{r warning= FALSE, message=FALSE }
#response  variable training
y_train<-data[train,3]
temp<-ifelse(y_train==0,1,0)
y_train<-cbind(y_train,temp)

#response  variable testing
y_test<-data[-train,3]
temp1<-ifelse(y_test==0,1,0)
y_test<-cbind(y_test,temp1)

# fit the autoencoder for training
hidden = c (10 ,10)
fit <- Rsda ( x_train , y_train , hidden )



```


## Train the model 0% noise
```{r warning= FALSE, message=FALSE }
#set noise level to 0
setCorruptionLevel ( fit , x = 0.0)
summary ( fit )

#other parameters to set
#setFinetuneEpochs
#setFinetuneLearningRate
#setPretrainLearningRate
#setPretrainEpochs
#1- pretrain
pretrain ( fit )
#2- fine tune
finetune ( fit )


```


## heck predict results 0% noise
```{r warning= FALSE, message=FALSE }
predProb <- predict ( fit , x_test )
head ( predProb ,6)
plot(predProb[,1], predProb[,2])
head ( y_test ,3)
```


## heck predict results
```{r warning= FALSE, message=FALSE }
pred1<-ifelse(predProb[,1]>=0.5,1,0)
table(pred1,y_test[,1], dnn=c("Predicted","Observed"))
```






## train a model 25% noise
```{r warning= FALSE, message=FALSE }
setCorruptionLevel(fit,x=0.25)
pretrain(fit)
finetune(fit)
predProb<-predict(fit,x_test)
pred1<-ifelse(predProb[,1]>=0.5,1,0)
table(pred1,y_test[,1],dnn=c("Predicted","Observed"))

```




## train a model 50% noise
```{r warning= FALSE, message=FALSE }
setCorruptionLevel(fit,x=0.50)
pretrain(fit)
finetune(fit)
predProb<-predict(fit,x_test)
pred1<-ifelse(predProb[,1]>=0.5,1,0)
table(pred1,y_test[,1],dnn=c("Predicted","Observed"))

```




















